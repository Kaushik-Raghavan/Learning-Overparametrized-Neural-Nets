{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWt_BSnhNtU_",
        "colab_type": "text"
      },
      "source": [
        "## Structure of code\n",
        "\n",
        "The code is structures as follows: \n",
        "\n",
        "* **Import module cell** : All the necessary modules are imported\n",
        "\n",
        "* **Logger class** : A class implemented to manage logging of values of variables that need to be tracked while training. For example, training error, test accuracy, change in activation pattern (in our case), etc\n",
        "\n",
        "* **Python generator function cell** : This cell has a class `StructuredDatasetGenerator` that initializes the parameters of the distribution from which data points need to sampled and has a generator method that samples data points from these distributions. This generator function is used in the `Model` class to build training and test dataset for the neural network\n",
        "\n",
        "* **Model class** : The most important cell on this notebook. This has class `Model`, which when initialized builds data t inpupipeline, a neural netowrk and pseudo network on the same temsorflow computation graph. \n",
        "\n",
        "* **Configurations** : This cell contains the class `Configuration` that has the specifications/hyper-parameters of experiment to be carried out.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67CqYeqInRZ",
        "colab_type": "text"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnvD_dEMpIkm",
        "colab_type": "code",
        "outputId": "fe0b26fb-22f1-4321-9222-818b4bed0ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial.distance import hamming\n",
        "from IPython import display\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.transform as im\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import inspect\n",
        "import random\n",
        "import struct\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMJ6d8CDQWSs",
        "colab_type": "code",
        "outputId": "14d625d2-5242-44a3-80d9-f2674db4487e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check if GPU is available\n",
        "from tensorflow.python.client import device_lib \n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "# print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK0TmzAjGmhs",
        "colab_type": "text"
      },
      "source": [
        "## Logger class\n",
        "\n",
        "TLDR : This is an object to keep track of values like train error, test error, etc - the typical values one would like to track while training a neural network. In our case, we would also track activation pattern and the magnitude of weights changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPGzviL-Glp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Logger:\n",
        "    \"\"\"Logger class can be used to log values of several parameters of the network during training. All the logs are stored\n",
        "    in a dataframe and the dataframe is updated when a new log comes in. By making `live_plot=True`, the live plots of the \n",
        "    values logged can also be observed.\n",
        "    \"\"\"\n",
        "    def __init__(self, var_names, include_step=True, live_plot=True):\n",
        "        \"\"\"Logger class initializer\n",
        "        Parameters:\n",
        "            `var_names` : list of variable names to track. These names will be the header row in the internal dataframe \n",
        "                          maintained by the Logger class\n",
        "            `include_step` : Boolean to indicate whether to include a 'step' column in the dataframe\n",
        "            `live_plot` : Boolean to indicate whether to show live plots of the variables as the logs get added\n",
        "        \"\"\"\n",
        "        self.var_names = var_names\n",
        "        self.include_step = include_step\n",
        "        if include_step and ('step' not in self.var_names):\n",
        "            self.var_names = ['step'] + self.var_names\n",
        "        self.dataframe = pd.DataFrame(columns=self.var_names)\n",
        "        self.num_vars = len(self.var_names)\n",
        "        self.num_steps = 0\n",
        "        self.plot_lists = {}\n",
        "        self.verbose = []\n",
        "        self.live_plot = live_plot\n",
        "        if self.live_plot:\n",
        "            print(self.var_names)\n",
        "            if self.num_vars < 3:\n",
        "                figsize = (7.0 * self.num_vars, 4.8)\n",
        "            else:\n",
        "                figsize = (21.0, 4.8 * (self.num_vars//3 + 1))\n",
        "            self.fig = plt.figure(figsize=figsize)\n",
        "            plt.subplots_adjust(wspace=0.3, hspace=0.4)\n",
        "            self._set_plots(self.var_names[1:])\n",
        "\n",
        "    def add_log(self, step, vals):\n",
        "        \"\"\" Adds entries to the dataframe\n",
        "        \"\"\"\n",
        "        if self.include_step:\n",
        "            self.dataframe.loc[self.num_steps] = [step] + vals\n",
        "        else:\n",
        "            self.dataframe.loc[self.num_steps] = vals\n",
        "        self.num_steps += 1\n",
        "        if self.live_plot:\n",
        "            self._visualize()\n",
        "\n",
        "    def add_verbose(self, v):\n",
        "        \"\"\" Adds verbose to the log. This log can be saved using `save` method implemented below in a text file 'verbose.txt' \n",
        "        \"\"\"\n",
        "        self.verbose.append(v)\n",
        "\n",
        "    def _visualize(self):\n",
        "        for yname in self.var_names[1:]:\n",
        "            plot_name = yname + '_vs_' + 'step'\n",
        "            ax = self.plot_lists[plot_name]\n",
        "            ax.clear()\n",
        "            ax.set_title(plot_name, color='w')\n",
        "            ax.set_xlabel('step', color='w')\n",
        "            ax.set_ylabel(yname, color='w')\n",
        "            #ax.set(ylabel=yname, xlabel='step', title=plot_name, color='b')\n",
        "            ax.plot(list(self.dataframe['step'].values), list(self.dataframe[yname].values))\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(self.fig)\n",
        "\n",
        "    def _set_plots(self, ynames, xnames='step'):\n",
        "        if isinstance(xnames, list):\n",
        "            assert len(xnames) == len(ynames)\n",
        "        else:\n",
        "            assert isinstance(xnames, str)\n",
        "            xnames = [xnames] * len(ynames)\n",
        "\n",
        "        num_plots = len(ynames)\n",
        "        for i, (yname, xname) in enumerate(zip(ynames, xnames)):    \n",
        "            plot_name = yname + '_vs_' + xname\n",
        "            # print(num_plots // 3 + 1)\n",
        "            self.plot_lists[plot_name] = self.fig.add_subplot(num_plots//3 + 1, 3, i+1)\n",
        "\n",
        "    def save(self, folder='.'):\n",
        "        self.dataframe.to_csv(os.path.join(folder, 'logs.csv'))\n",
        "        np.savetxt(os.path.join(folder, 'verbose.txt'), self.verbose, delimiter='\\n', fmt='%s')\n",
        "\n",
        "    def close(self):\n",
        "        display.clear_output(wait=False)\n",
        "\n",
        "\n",
        "# TEST for Logger\n",
        "\n",
        "# vars = ['a', 'b', 'c', 'd']\n",
        "# ckpt = Logger(vars)   \n",
        "# print(vars)\n",
        "# for i in range(10):\n",
        "#     vals = np.random.rand(4)\n",
        "#     ckpt.add_log(i * 2, [y for x, y in zip(vars, vals)]) \n",
        "#     ckpt._visualize()\n",
        "#     time.sleep(1)\n",
        "# ckpt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsSWHYFcsQA-",
        "colab_type": "text"
      },
      "source": [
        "## Generator function to generate data points\n",
        "\n",
        "This cell has a function that generates samples from a structured dataset. The parameter values inside the functions are follow the specifications of synthetic data from the original paper \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Us8cbwb2Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructuredDatasetGenerator:\n",
        "    \"\"\" This object defines a mixture of gaussiand from which samples are to be generated.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_dim=1000, n_classes=10, n_components_per_class=2, n_samples=4000, component_probability=None):\n",
        "        \"\"\" The constructor creates a `StructuredDatasetGenerator` object. It uses the arguments to create dataset having \n",
        "        `n_classes` classes, `n_components_per_class` distinct gaussian distributions per class. If the `component_probability` \n",
        "        is `None`, uniform probability value will be assigned to each component. The constructor generates `n_samples` data \n",
        "        points from these mixture of gaussians before exiting. The samples generated are drawn from a mixture of \n",
        "        `n_classes` * `n_components_per_class` gaussian distributions.\n",
        "        \"\"\"\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dim = n_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_components_per_class = n_components_per_class\n",
        "        self.component_probability = component_probability\n",
        "\n",
        "        if component_probability is None:\n",
        "            # if the component probabilites are not given, assign uniform probabilities to all components\n",
        "            p_ij = 1.0 / (n_classes * n_components_per_class)\n",
        "            self.component_probability = [p_ij for _ in range(n_components_per_class * n_classes)]\n",
        "    \n",
        "        _sigma = 1.0    # specified in paper\n",
        "        _sigma0 = 5.0   # specified in paper\n",
        "    \n",
        "        self.distributions = []  # stores the mean and std deviation\n",
        "        for i in range(n_classes):\n",
        "            distributions_per_class = []\n",
        "            for j in range(n_components_per_class):\n",
        "                mu = np.random.normal(loc=0.0, scale=(_sigma0 / np.sqrt(n_dim)), size=n_dim)\n",
        "                cov = (_sigma**2 / n_dim) * np.identity(n_dim)\n",
        "                distributions_per_class.append((mu, cov))\n",
        "            self.distributions.append(distributions_per_class)\n",
        "\n",
        "        # Samples generated beforehand. The `self.generate` method will yeild these generated samples\n",
        "        self.samples = []\n",
        "        for i in range(n_samples):\n",
        "            comp_chosen = np.argmax(np.random.multinomial(1, self.component_probability))\n",
        "            curr_class = int(comp_chosen / self.n_components_per_class)\n",
        "            curr_comp = int(comp_chosen % self.n_components_per_class)\n",
        "    \n",
        "            mu, cov = self.distributions[curr_class][curr_comp]\n",
        "            curr_sample = np.random.multivariate_normal(mean=mu, cov=cov)\n",
        "            curr_sample = curr_sample / np.linalg.norm(curr_sample)\n",
        "            self.samples.append((curr_sample, curr_class))\n",
        "\n",
        "\n",
        "    def generate(self, n_samples=-1):\n",
        "        if n_samples == -1:\n",
        "            n_samples = self.n_samples\n",
        "        for i in range(n_samples):\n",
        "            yield self.samples[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SErQB8fxxcjC",
        "colab_type": "text"
      },
      "source": [
        "## Model class\n",
        "\n",
        "The biggest part of the code. In this cell the `Model` class is implemented that loads in the dataset, generates input pipeline for neural net, builds a neural net. It has trainig and testing modules for training and testing the build network on the loaded dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mPhFJkkq0rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, cfg, graph=None):\n",
        "        \"\"\" The constructor takes in a configuration specifying the parameters of dataset to be prepared and neural network to be \n",
        "        built.\n",
        "        \"\"\"\n",
        "        self.input = None\n",
        "        self.cfg = cfg\n",
        "        self.activation_fn = cfg.activation_fn\n",
        "        if graph is None:\n",
        "            self.comp_graph = tf.Graph()\n",
        "        else:\n",
        "            self.comp_graph = graph\n",
        "        self.sess = tf.Session(graph=self.comp_graph)\n",
        "\n",
        "        with self.comp_graph.as_default():\n",
        "            # Building input pipeline\n",
        "            self._build_dataset()\n",
        "            self.is_training = tf.placeholder(dtype=tf.bool, name='is_in_training_mode')\n",
        "            self.fixed_order_inputs = tf.placeholder(dtype=tf.bool, name='input_fixed_order_dataset')\n",
        "            \n",
        "            # Building neural net architecture\n",
        "            self._build_model(cfg.layer_sizes)\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "            self._save_initial_weights()\n",
        "\n",
        "            # Building pseudo net architecture - neural network where activation patterns are made to be fixed while training\n",
        "            with tf.variable_scope('PseudoNet', reuse=tf.AUTO_REUSE):\n",
        "                self._build_pseudo_model(cfg.layer_sizes)\n",
        "            self._initialize_pseudo_net()\n",
        "\n",
        "            self.saver = tf.train.Saver()\n",
        "\n",
        "        # Initializing the values of variables to track while training\n",
        "        self.change_in_weights = 0.0\n",
        "        self.frac_activation_changed = 0.0\n",
        "        self.initial_pattern = self._get_activation_pattern()\n",
        "\n",
        "        w, b = self.get_weight_val('DenseLayer_1')\n",
        "        with tf.variable_scope('PseudoNet', reuse=tf.AUTO_REUSE):\n",
        "            pw, pb = self.get_weight_val('DenseLayer_1')\n",
        "\n",
        "        assert((w == pw).all() and (b == pb).all())\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        \"\"\" Builds input dataset pipelene in the tensorflow graph\n",
        "        \"\"\"\n",
        "        with self.comp_graph.as_default():\n",
        "            # Generating dataset\n",
        "            generator_args = [self.cfg.n_dim, self.cfg.n_classes, self.cfg.n_comp]\n",
        "            dataset = StructuredDatasetGenerator(*generator_args)\n",
        "            self._dataset = tf.data.Dataset.from_generator(dataset.generate, \n",
        "                                                           output_types=(tf.float32, tf.float32),\n",
        "                                                           output_shapes=([self.cfg.n_dim,], []), \n",
        "                                                           )\n",
        "             \n",
        "            # Splitting dataset into train and test\n",
        "            _train_dataset = self._dataset.take(self.cfg.n_train_samples)\n",
        "            _test_dataset = self._dataset.skip(self.cfg.n_train_samples)\n",
        "            \n",
        "            # Creating training dataset\n",
        "            # Dataset will be shuffled after every epoch and then split into batches\n",
        "            self.train_dataset = _train_dataset.shuffle(self.cfg.n_train_samples)\\\n",
        "                                               .batch(self.cfg.batch_size)\\\n",
        "                                               .repeat()\n",
        "\n",
        "            # No shuffling of dataset - This dataset is used to keep track of activation patterns of hidden layer\n",
        "            self._ordered_dataset_batch_size = self.cfg.n_train_samples\n",
        "            self.fixed_order_train_dataset = _train_dataset.batch(self._ordered_dataset_batch_size)\\\n",
        "                                                           .repeat()\n",
        "\n",
        "            # Test dataset\n",
        "            self.test_dataset = _test_dataset.take(self.cfg.n_test_samples)\\\n",
        "                                             .batch(self.cfg.n_test_samples)\\\n",
        "                                             .repeat()\n",
        "\n",
        "            _data_iter = tf.data.make_one_shot_iterator(self.train_dataset)\n",
        "            self.inputs_batch, self.gt_batch = _data_iter.get_next()\n",
        "            _fixed_order_iter = tf.data.make_one_shot_iterator(self.fixed_order_train_dataset)\n",
        "            self.ordered_inputs, self.ordered_gt = _fixed_order_iter.get_next()\n",
        "            _test_iter = tf.data.make_one_shot_iterator(self.test_dataset)\n",
        "            self.test_inputs, self.test_gt = _test_iter.get_next()\n",
        "\n",
        "    def _build_model(self, layer_sizes=None):\n",
        "        \"\"\" Builds neural network model in the tensorflow graph\n",
        "        \"\"\"\n",
        "        # Preparing input and ground truth tensors according to the flags `self.is_training`, `self.fixed_order_inputs`\n",
        "        outp = tf.cond(self.is_training, lambda: tf.cond(self.fixed_order_inputs, lambda: self.ordered_inputs,\n",
        "                                                                                  lambda: self.inputs_batch),\n",
        "                                         lambda: self.test_inputs)\n",
        "        outp.set_shape(tf.TensorShape([None, self.cfg.n_dim]))\n",
        "        outp = tf.expand_dims(outp, axis=1)\n",
        "        gt = tf.cond(self.is_training, lambda: tf.cond(self.fixed_order_inputs, lambda: self.ordered_gt,\n",
        "                                                                                lambda: self.gt_batch),\n",
        "                                       lambda: self.test_gt)\n",
        "        if self.cfg.task == 'classification':\n",
        "            gt.set_shape(tf.TensorShape([None,]))\n",
        "        else:\n",
        "            gt.set_shape(tf.TensorShape([None, self.cfg.layer_sizes[-1]]))\n",
        "\n",
        "        # Building network \n",
        "        self.layer_outputs = []\n",
        "        self.layer_outputs.append(tf.squeeze(outp))\n",
        "        prev_sz = layer_sizes[0]\n",
        "        for layer_idx, sz in enumerate(layer_sizes[1:-1]):\n",
        "            with tf.variable_scope('DenseLayer_{}'.format(layer_idx + 1), reuse=tf.AUTO_REUSE):\n",
        "                w = tf.get_variable(name='kernel', shape=(prev_sz, sz), dtype=tf.float32,\n",
        "                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0 / np.sqrt(sz)),\n",
        "                                    trainable=True)\n",
        "                b = tf.get_variable(name='bias', shape=(sz,), dtype=tf.float32,\n",
        "                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0 / np.sqrt(sz)),\n",
        "                                    trainable=True)\n",
        "                outp = tf.add(tf.matmul(outp, w), b)\n",
        "                outp = self.activation_fn(outp)\n",
        "                self.layer_outputs.append(tf.squeeze(outp))\n",
        "            prev_sz = sz\n",
        "\n",
        "        with tf.variable_scope('OutputLayer', reuse=tf.AUTO_REUSE):\n",
        "            sz = layer_sizes[-1]\n",
        "            w = tf.get_variable(name='kernel', shape=(prev_sz, sz), dtype=tf.float32,\n",
        "                                initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0),\n",
        "                                trainable=self.cfg.trainable_output_weights)\n",
        "            b = tf.get_variable(name='bias', shape=(sz,), dtype=tf.float32,\n",
        "                                initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0),\n",
        "                                trainable=self.cfg.trainable_output_weights)\n",
        "            self.output = tf.add(tf.matmul(outp, w), b)\n",
        "\n",
        "        self.output = tf.squeeze(self.output)\n",
        "        self.layer_outputs.append(self.output)\n",
        "\n",
        "        # Defining post-output operations: finding loss, back propagation, evaluating accuracy of predictions, etc\n",
        "        self.loss_op = self.get_loss(labels=gt, predictions=self.output)\n",
        "        self.gradients = self._create_gradient_ops(self.loss_op, layer_sizes)\n",
        "\n",
        "        if self.cfg.task == 'classification':\n",
        "            self.pred_output = tf.arg_max(self.output, dimension=-1)\n",
        "            # _, self.accuracy_op = tf.metrics.accuracy(labels=gt, predictions=self.pred_output, name='accuracy_op')\n",
        "\n",
        "        self.opt = self.cfg.optimizer(learning_rate=self.cfg.learning_rate, name='Optimizer')\n",
        "        self.train_op = self.opt.minimize(self.loss_op, global_step=tf.train.get_global_step(), name='training_operations')\n",
        "\n",
        "    def _build_pseudo_model(self, layer_sizes=None):\n",
        "        \"\"\" Builds the pseudo neural network model in the tensorflow graph\n",
        "        \"\"\"\n",
        "        # Preparing input and ground truth tensors according to the flags `self.is_training`, `self.fixed_order_inputs`\n",
        "        outp = tf.cond(self.is_training, lambda: tf.cond(self.fixed_order_inputs, lambda: self.ordered_inputs,\n",
        "                                                                                  lambda: self.inputs_batch),\n",
        "                                         lambda: self.test_inputs)\n",
        "        outp.set_shape(tf.TensorShape([None, self.cfg.n_dim]))\n",
        "        outp = tf.expand_dims(outp, axis=1)\n",
        "        gt = tf.cond(self.is_training, lambda: tf.cond(self.fixed_order_inputs, lambda: self.ordered_gt,\n",
        "                                                                                lambda: self.gt_batch),\n",
        "                                       lambda: self.test_gt)\n",
        "        if self.cfg.task == 'classification':\n",
        "            gt.set_shape(tf.TensorShape([None,]))\n",
        "        else:\n",
        "            gt.set_shape(tf.TensorShape([None, self.cfg.layer_sizes[-1]]))\n",
        "\n",
        "        # Building pseudo network \n",
        "        self.p_layer_outputs = []\n",
        "        self.p_layer_outputs.append(tf.squeeze(outp))\n",
        "        prev_sz = layer_sizes[0]\n",
        "        for layer_idx, sz in enumerate(layer_sizes[1:-1]):\n",
        "            with tf.variable_scope('DenseLayer_{}'.format(layer_idx + 1), reuse=tf.AUTO_REUSE):\n",
        "                setattr(self, 'p_w_init_{}'.format(layer_idx + 1), \n",
        "                        tf.placeholder(dtype=tf.float32, shape=[prev_sz, sz], name='Initial_weight_{}'.format(layer_idx + 1)))\n",
        "                setattr(self, 'p_b_init_{}'.format(layer_idx + 1), \n",
        "                        tf.placeholder(dtype=tf.float32, shape=[sz,], name='Initial_bias_{}'.format(layer_idx + 1)))\n",
        "                w = tf.get_variable(name='kernel', dtype=tf.float32,\n",
        "                                    initializer=getattr(self, 'p_w_init_{}'.format(layer_idx + 1)),\n",
        "                                    trainable=True)\n",
        "                b = tf.get_variable(name='bias', dtype=tf.float32,\n",
        "                                    initializer=getattr(self, 'p_b_init_{}'.format(layer_idx + 1)),\n",
        "                                    trainable=True)\n",
        "                _outp = tf.add(tf.matmul(outp, w), b)\n",
        "                _init_dotproduct = tf.add(tf.matmul(outp, getattr(self, 'p_w_init_{}'.format(layer_idx + 1))),\n",
        "                                          getattr(self, 'p_b_init_{}'.format(layer_idx + 1)))\n",
        "                _pseudo_relu_state = tf.math.sign(_init_dotproduct)\n",
        "                _pseudo_relu_state = (_pseudo_relu_state + 1.0) / 2.0\n",
        "                outp = _outp * _pseudo_relu_state\n",
        "                self.p_layer_outputs.append(tf.squeeze(outp))\n",
        "            prev_sz = sz\n",
        "\n",
        "        with tf.variable_scope('OutputLayer', reuse=tf.AUTO_REUSE):\n",
        "            sz = layer_sizes[-1]\n",
        "            setattr(self, 'p_w_init_output', \n",
        "                    tf.placeholder(dtype=tf.float32, shape=[prev_sz, sz], name='Initial_weight_output'))\n",
        "            setattr(self, 'p_b_init_output', \n",
        "                    tf.placeholder(dtype=tf.float32, shape=[sz,], name='Initial_bias_output'))\n",
        "            w = tf.get_variable(name='kernel', dtype=tf.float32,\n",
        "                                initializer=getattr(self, 'p_w_init_output'),\n",
        "                                trainable=self.cfg.trainable_output_weights)\n",
        "            b = tf.get_variable(name='bias', dtype=tf.float32,\n",
        "                                initializer=getattr(self, 'p_b_init_output'),\n",
        "                                trainable=self.cfg.trainable_output_weights)\n",
        "            self.p_output = tf.add(tf.matmul(outp, w), b)\n",
        "\n",
        "        self.p_output = tf.squeeze(self.p_output)\n",
        "        self.p_layer_outputs.append(self.p_output)\n",
        "\n",
        "        # Defining post-output operations: finding loss, back propagation, evaluating accuracy of predictions, etc\n",
        "        self.p_loss_op = self.get_loss(labels=gt, predictions=self.p_output)\n",
        "        self.p_gradients = self._create_gradient_ops(self.p_loss_op, layer_sizes)\n",
        "\n",
        "        if self.cfg.task == 'classification':\n",
        "            self.p_pred_output = tf.arg_max(self.p_output, dimension=-1)\n",
        "\n",
        "        self.p_opt = self.cfg.optimizer(learning_rate=self.cfg.learning_rate, name='Optimizer')\n",
        "        self.p_train_op = self.p_opt.minimize(self.p_loss_op, global_step=tf.train.get_global_step(), name='training_operations')\n",
        "        \n",
        "    def _initialize_pseudo_net(self):\n",
        "        self.feed_dict = {}\n",
        "        with tf.variable_scope('PseudoNet', reuse=tf.AUTO_REUSE):\n",
        "            for layer_idx, sz in enumerate(self.cfg.layer_sizes[1:-1]):\n",
        "                with tf.variable_scope('DenseLayer_{}'.format(layer_idx + 1), reuse=tf.AUTO_REUSE):\n",
        "                    self.feed_dict[getattr(self, 'p_w_init_{}'.format(layer_idx + 1))] = self.initial_weights[layer_idx][0]\n",
        "                    self.feed_dict[getattr(self, 'p_b_init_{}'.format(layer_idx + 1))] = self.initial_bias[layer_idx]\n",
        "            with tf.variable_scope('OutptuLayer'):\n",
        "                self.feed_dict[getattr(self, 'p_w_init_output')] = self.initial_weights[-1][0]\n",
        "                self.feed_dict[getattr(self, 'p_b_init_output')] = self.initial_bias[-1]\n",
        "\n",
        "        var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='PseudoNet')\n",
        "        self.sess.run(tf.initialize_variables(var_list), feed_dict=self.feed_dict)\n",
        "\n",
        "    def _save_initial_weights(self):\n",
        "        with self.comp_graph.as_default():\n",
        "            self.initial_weights = []\n",
        "            self.initial_bias = []\n",
        "            for layer_idx, sz in enumerate(self.cfg.layer_sizes[1:-1]):\n",
        "                w, b = self.get_weight_val('DenseLayer_' + str(layer_idx + 1))\n",
        "                self.initial_weights.append((w, np.linalg.norm(w)))\n",
        "                self.initial_bias.append(b)\n",
        "            w, b = self.get_weight_val('OutputLayer')\n",
        "            self.initial_weights.append((w, np.linalg.norm))\n",
        "            self.initial_bias.append(b)\n",
        "\n",
        "    def _get_activation_pattern(self):\n",
        "        \"\"\" Returns a list of strings of size equal to number of hidden layers. The length of each string will be equal to the \n",
        "        number of training samples in the dataset. Each string will be made up of either '1' indicating activated node (positive \n",
        "        value after ReLU operation) or '0' indicating dead node (0 after ReLU operation) for that input\n",
        "        \"\"\"\n",
        "        with self.comp_graph.as_default():\n",
        "            _pattern = []\n",
        "            _activation_states = []\n",
        "            _num_batches = int(self.cfg.n_train_samples / self._ordered_dataset_batch_size)\n",
        "            for data_idx in range(_num_batches):\n",
        "                feed_dict = {self.is_training: True, self.fixed_order_inputs: True}\n",
        "                # gets a matrix `_activations` of size(batch_size, hidden_layer_size)) \n",
        "                # storing activations for the current batchof inputs\n",
        "                _activations = self.sess.run(self.layer_outputs[1], feed_dict=feed_dict)\n",
        "                _activations = np.squeeze(np.where(_activations > 0.0, 1, 0))   \n",
        "                _activation_states.append(_activations)\n",
        "            _activation_states = np.concatenate(_activation_states, axis=0).T  # new shape (hidden_layer_size, num_samples)\n",
        "            # _pattern = np.array([''.join(st) for st in _activation_states])\n",
        "            return _activation_states\n",
        "\n",
        "    def _create_gradient_ops(self, loss_op, layer_sizes):\n",
        "        prev_sz = layer_sizes[0]\n",
        "        gradients = []\n",
        "        for layer_idx, sz in enumerate(layer_sizes[1:-1]):\n",
        "            with tf.variable_scope('DenseLayer_{}'.format(layer_idx + 1), reuse=tf.AUTO_REUSE):\n",
        "                _w = tf.get_variable('kernel'.format(layer_idx + 1))\n",
        "                _grad = tf.gradients(loss_op, _w, name='Gradient_wrt_Weight_{}'.format(layer_idx + 1))\n",
        "                gradients.append(_grad)\n",
        "        return gradients\n",
        "\n",
        "    def get_loss(self, labels, predictions):\n",
        "        \"\"\" Loss function for the neural network\n",
        "        \"\"\"\n",
        "        with self.comp_graph.as_default():\n",
        "            if self.cfg.task == 'classification':\n",
        "                loss_mean = self.cfg.loss_fn(labels=tf.cast(labels, dtype=tf.int32), logits=predictions)\n",
        "            else:\n",
        "                loss_mean = self.cfg.loss_fn(labels=tf.cast(labels, dtype=predictions.dtype), predictions=predictions)\n",
        "            loss_mean = tf.reduce_mean(loss_mean)\n",
        "            return loss_mean\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\" This method trains both the true and pseudo neural network\n",
        "        \"\"\"\n",
        "        # Variables to track and log while training\n",
        "        vars_to_log = ['train_loss', 'p_train_loss', 'frac_activation_changed', 'change_in_weights', 'test_accuracy', \n",
        "                       'grad_coupling']\n",
        "        ckpt = Logger(vars_to_log, live_plot=False)\n",
        "        global_step = 0\n",
        "\n",
        "        tik = time.time()\n",
        "        avg_time_taken = 0.0\n",
        "        for step in range(self.cfg.max_steps):\n",
        "            global_step += 1\n",
        "            with self.comp_graph.as_default():\n",
        "                feed_dict = {self.is_training: True, self.fixed_order_inputs: False}\n",
        "                feed_dict.update(self.feed_dict)\n",
        "                vars_to_fetch = [self.train_op, self.p_train_op, self.loss_op, self.p_loss_op]\n",
        "                tik = time.time()\n",
        "                _, _, self.train_loss, self.p_train_loss = self.sess.run(vars_to_fetch, feed_dict=feed_dict)\n",
        "                tok = time.time()\n",
        "                avg_time_taken += float(tok - tik)\n",
        "            verbose = \"[Step {}/{}] Training loss = {:.5f}\".format(step + 1, self.cfg.max_steps, self.train_loss)\n",
        "\n",
        "            if step % self.cfg.log_step == 0:   # log values of variables to track\n",
        "                verbose += \"  (Time taken = {:.3f}sec)\".format(avg_time_taken / self.cfg.log_step)\n",
        "                avg_time_taken = 0.0\n",
        "                tik = time.time()\n",
        "\n",
        "                # Computing changes activation pattern\n",
        "                curr_activ_pattern = self._get_activation_pattern()\n",
        "                def hamming_dist(arr1, arr2):\n",
        "                    if isinstance(arr1, list) or isinstance(arr1, np.ndarray):\n",
        "                        return np.array([hamming(s1, s2) for s1, s2 in zip(arr1, arr2)])\n",
        "                    return hamming(arr1, arr2)\n",
        "                _thresh = 0.20   # Threshold for hamming distance between two activation pattern\n",
        "                # Two activation patterns will be considered same if their hamming distance is less than `_thresh`\n",
        "                _num_changed = np.sum(np.where(hamming_dist(curr_activ_pattern, self.initial_pattern) <= _thresh, 0, 1))\n",
        "                self.frac_activation_changed = _num_changed / len(curr_activ_pattern)\n",
        "\n",
        "                # Computing change in weights\n",
        "                _current_weights, _curr_bias = self.get_weight_val('DenseLayer_1')\n",
        "                _init_weights, _init_norm = self.initial_weights[0]\n",
        "                self.change_in_weights = np.linalg.norm(_current_weights - _init_weights) / _init_norm\n",
        "\n",
        "                feed_dict = {self.is_training: False, self.fixed_order_inputs: False}\n",
        "                feed_dict.update(self.feed_dict)\n",
        "                vars_to_fetch = [self.pred_output, self.test_gt, self.gradients[0], self.p_gradients[0]]\n",
        "                pred, gt, grad, p_grad = self.sess.run(vars_to_fetch, feed_dict=feed_dict)\n",
        "\n",
        "                # Calculating test accuracy\n",
        "                num_correct = np.sum(np.where(pred == gt, 1, 0))\n",
        "                self.test_accuracy = num_correct / len(pred)\n",
        "\n",
        "                # Calculating difference between true gradient and pseudo gradient\n",
        "                grad, p_grad = grad[0], p_grad[0]\n",
        "                self.grad_coupling = np.linalg.norm(grad - p_grad) / np.linalg.norm(p_grad)\n",
        "\n",
        "                vals_of_vars = [getattr(self, var) for var in vars_to_log]\n",
        "                ckpt.add_log(step, vals_of_vars)\n",
        "                tok = time.time()\n",
        "                verbose += \"  (Time taken for logging = {:.3f}sec)\".format(float(tok - tik))\n",
        "                ckpt.add_verbose(verbose)\n",
        "                print(verbose)\n",
        "\n",
        "            if (step + 1) % self.cfg.ckpt_step == 0:   # evaluate model on test set and save model\n",
        "                if self.cfg.test:\n",
        "                    self.test(1)\n",
        "\n",
        "                if self.cfg.save_model:\n",
        "                    model_dir = os.path.join(self.cfg.expt_dir, self.cfg.expt_name)\n",
        "                    if not os.path.exists(model_dir):\n",
        "                        os.mkdir(model_dir)\n",
        "                    if not os.path.exists(os.path.join(model_dir, 'models')):\n",
        "                        os.mkdir(os.path.join(model_dir, 'models'))\n",
        "                    with self.comp_graph.as_default():\n",
        "                        self.saver.save(self.sess, os.path.join(model_dir, 'models/model'), global_step=step)\n",
        "                    verbose = \"Checkpoint created after {} steps\".format(step + 1)\n",
        "                    ckpt.add_verbose(verbose)\n",
        "                    print(verbose)\n",
        "\n",
        "                model_dir = os.path.join(self.cfg.expt_dir, self.cfg.expt_name)\n",
        "                if not os.path.exists(model_dir):\n",
        "                    os.mkdir(model_dir)\n",
        "                ckpt.save(model_dir)\n",
        "\n",
        "            if step == self.cfg.max_steps - 1:\n",
        "                curr_w, curr_b = self.get_weight_val('DenseLayer_1')\n",
        "                init_w, init_b = self.initial_weights[0]\n",
        "                diff_w = curr_w - init_w\n",
        "                self.diff_eigvals = sorted(np.linalg.eigvals(diff_w.dot(diff_w.T)))\n",
        "                self.final_eigvals = sorted(np.linalg.eigvals(curr_w.dot(curr_w.T))) \n",
        "                _combined = np.stack([self.diff_eigvals, self.final_eigvals], axis=0).T\n",
        "                np.savetxt(os.path.join(model_dir, 'eigen_vals.txt'), _combined)\n",
        "\n",
        "    def test(self, n_batches):\n",
        "        with self.comp_graph.as_default():\n",
        "            print(\"\\nTesting model on test dataset ...\")\n",
        "            tik = time.time()\n",
        "            accuracy = 0.0\n",
        "            for data_idx in range(n_batches):\n",
        "                feed_dict = {self.is_training: False, self.fixed_order_inputs: False}\n",
        "                feed_dict.update(self.feed_dict)\n",
        "                pred, gt = self.sess.run([self.pred_output, self.test_gt], feed_dict=feed_dict)\n",
        "                num_correct = np.sum(np.where(pred == gt, 1, 0))\n",
        "                acc = num_correct / len(pred)\n",
        "                accuracy += acc;\n",
        "            accuracy /= n_batches\n",
        "            tok = time.time()\n",
        "            print(\"Accuracy of prediction on test data = {:.2f}%\".format(accuracy * 100.0))\n",
        "            print(\"Time taken for testing = {:.3f}sec\\n\".format(float(tok - tik)))\n",
        "\n",
        "    def score(self, test_images, test_labels):\n",
        "        with self.comp_graph.as_default():\n",
        "            feed_dict = {self.input_batch: test_images, self.gt_batch: test_labels, \n",
        "                         self.is_training: False, self.fixed_order_inputs: False}\n",
        "            feed_dict.update(self.feed_dict)\n",
        "            pred = self.sess.run(self.pred_output, feed_dict=feed_dict)\n",
        "            num_correct = np.sum(np.where(pred == test_lables, 1, 0))\n",
        "            test_accuracy = num_correct / len(pred)\n",
        "        return test_accuracy\n",
        "\n",
        "    def predict(self, inputs, one_hot_output=False):\n",
        "        with self.comp_graph.as_default():\n",
        "            feed_dict = {self.test_inputs: inputs, self.is_training: False, self.fixed_order_inputs: False}\n",
        "            feed_dict.update(self.feed_dict)\n",
        "            out = self.sess.run(self.pred_output, feed_dict=feed_dict)\n",
        "            return out\n",
        "\n",
        "    def load(self, model_dir):\n",
        "        \"\"\" Loads the model stored at the latest check point in the given directory \"\"\"\n",
        "        with self.comp_graph.as_default():\n",
        "            latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "            saver = tf.train.Saver()\n",
        "            self.sess.run(tf.local_variables_initializer())\n",
        "            saver.restore(self.sess, latest_checkpoint)\n",
        "\n",
        "    def get_layer_val(self, inputs, layer_idx, unit_idx=0):\n",
        "        assert (layer_idx < len(self.cfg.layer_sizes))\n",
        "        with self.comp_graph.as_default():\n",
        "            feed_dict = {self.test_inputs: inputs, self.is_training: False, self.fixed_order_inputs: False}\n",
        "            feed_dict.update(self.feed_dict)\n",
        "            layer_output = self.sess.run(self.layer_outputs[layer_idx], feed_dict=feed_dict)\n",
        "\n",
        "        if isinstance(unit_idx, int):\n",
        "            return layer_output[:, unit_idx]\n",
        "        elif isinstance(unit_idx, tuple) or isinstance(unit_idx, list):\n",
        "            assert(len(unit_idx) == 2)\n",
        "            assert(unit_idx[0] >= 0 and unit_idx[0] < unit_idx[1] and unit_idx[1] < len(layer_output))\n",
        "            return layer_output[: ,unit_idx[0]: unit_idx[1]]\n",
        "\n",
        "        raise TypeError(\"Wrong type of argument `unit_idx` passed to `get_layer_val` function.\\n\" + \\\n",
        "                        \"Expected either an int or a list or tuple of size 2\")\n",
        "        return \n",
        "\n",
        "    def get_weight_val(self, layer_name):\n",
        "        kernel_val, bias_val = None, None\n",
        "        kernel_val = self.comp_graph.get_tensor_by_name(layer_name + '/kernel:0').eval(session=self.sess)\n",
        "        bias_val = self.comp_graph.get_tensor_by_name(layer_name + '/bias:0').eval(session=self.sess)\n",
        "        return kernel_val, bias_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PWZ2S08xQwV",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c3H270GtCft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Configuration:\n",
        "    \"\"\" Class containing the parameters that design each experiment.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.expt_name = 'Experiment1'\n",
        "        self.expt_dir = 'drive/My Drive/CS7020_MiniProject'\n",
        "        self.task = 'classification'\n",
        "\n",
        "        # Dataset properties\n",
        "        self.n_train_samples = 1000\n",
        "        self.n_test_samples = 1000\n",
        "        self.n_classes = 10\n",
        "        self.n_comp = 2\n",
        "        self.n_dim = 100\n",
        "\n",
        "        # Architecture design\n",
        "        self.layer_sizes = [self.n_dim, 5000, self.n_classes]\n",
        "        self.activation_fn = 'relu'\n",
        "        self.loss_fn = 'sparse_softmax_cross_entropy_with_logits'\n",
        "\n",
        "        # Training configuration\n",
        "        self.trainable_output_weights = False\n",
        "        self.learning_rate = 10.0 / self.layer_sizes[1]\n",
        "        self.batch_size = 16\n",
        "        self.max_steps = 400\n",
        "        self.optimizer = 'sgd'\n",
        "        self.test = False\n",
        "\n",
        "        self.log_step = 5\n",
        "        self.ckpt_step = 50\n",
        "        self.save_model = True\n",
        "\n",
        "        self.comments = \"Testing with hamming distance thresh 0.20\"\n",
        "\n",
        "    def process(self):\n",
        "        # activation function\n",
        "        if hasattr(tf.nn, self.activation_fn):\n",
        "            self.activation_fn = getattr(tf.nn, self.activation_fn)\n",
        "        elif hasattr(tf.math, self.activation_fn):\n",
        "            self.activation_fn = getattr(tf.math, self.activation_fn)\n",
        "\n",
        "        # loss function\n",
        "        if hasattr(tf.losses, self.loss_fn):\n",
        "            self.loss_fn = getattr(tf.losses, self.loss_fn)\n",
        "        elif hasattr(tf.math, self.loss_fn):\n",
        "            self.loss_fn = getattr(tf.math, self.loss_fn)\n",
        "        elif hasattr(tf.nn, self.loss_fn):\n",
        "            self.loss_fn = getattr(tf.nn, self.loss_fn)\n",
        "\n",
        "        # optimizer\n",
        "        if self.optimizer.lower() == 'sgd':\n",
        "            self.optimizer = 'GradientDescent'\n",
        "        self.optimizer += 'Optimizer'\n",
        "        if hasattr(tf.train, self.optimizer):\n",
        "            self.optimizer = getattr(tf.train, self.optimizer)\n",
        "        else:\n",
        "            print(\"{} optimizer doesnt exist in tensorflow\".format(self.optimizer))\n",
        "\n",
        "    def save(self):\n",
        "        model_dir = os.path.join(self.expt_dir, self.expt_name)\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.mkdir(model_dir)\n",
        "        lst = []\n",
        "        for keys, vals in Configuration().__dict__.items():\n",
        "            curr_line = keys + \" : {}\".format(vals)\n",
        "            lst.append(curr_line)\n",
        "        np.savetxt(os.path.join(model_dir, 'config.txt'), lst, fmt='%s', delimiter='\\n')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1USLzaDCv8_",
        "colab_type": "text"
      },
      "source": [
        "## Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FZa-NRhAv4A",
        "colab_type": "code",
        "outputId": "42e90bb5-8cea-44f7-eb52-2782953c6007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config = Configuration()\n",
        "config.process()\n",
        "config.save()\n",
        "model = Model(config)\n",
        "model.train()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Step 1/400] Training loss = 3.42891  (Time taken = 0.112sec)  (Time taken for logging = 2.447sec)\n",
            "[Step 6/400] Training loss = 1.59808  (Time taken = 0.471sec)  (Time taken for logging = 2.096sec)\n",
            "[Step 11/400] Training loss = 0.75850  (Time taken = 0.460sec)  (Time taken for logging = 2.205sec)\n",
            "[Step 16/400] Training loss = 0.50267  (Time taken = 0.463sec)  (Time taken for logging = 2.110sec)\n",
            "[Step 21/400] Training loss = 0.39187  (Time taken = 0.466sec)  (Time taken for logging = 2.084sec)\n",
            "[Step 26/400] Training loss = 0.29529  (Time taken = 0.472sec)  (Time taken for logging = 2.154sec)\n",
            "[Step 31/400] Training loss = 0.16904  (Time taken = 0.480sec)  (Time taken for logging = 2.208sec)\n",
            "[Step 36/400] Training loss = 0.16037  (Time taken = 0.463sec)  (Time taken for logging = 2.051sec)\n",
            "[Step 41/400] Training loss = 0.14047  (Time taken = 0.460sec)  (Time taken for logging = 2.163sec)\n",
            "[Step 46/400] Training loss = 0.10975  (Time taken = 0.511sec)  (Time taken for logging = 2.151sec)\n",
            "Checkpoint created after 50 steps\n",
            "[Step 51/400] Training loss = 0.09398  (Time taken = 0.467sec)  (Time taken for logging = 2.136sec)\n",
            "[Step 56/400] Training loss = 0.07798  (Time taken = 0.478sec)  (Time taken for logging = 2.109sec)\n",
            "[Step 61/400] Training loss = 0.07554  (Time taken = 0.462sec)  (Time taken for logging = 2.148sec)\n",
            "[Step 66/400] Training loss = 0.06804  (Time taken = 0.529sec)  (Time taken for logging = 2.405sec)\n",
            "[Step 71/400] Training loss = 0.06224  (Time taken = 0.579sec)  (Time taken for logging = 2.344sec)\n",
            "[Step 76/400] Training loss = 0.06232  (Time taken = 0.518sec)  (Time taken for logging = 2.150sec)\n",
            "[Step 81/400] Training loss = 0.05732  (Time taken = 0.460sec)  (Time taken for logging = 2.209sec)\n",
            "[Step 86/400] Training loss = 0.05092  (Time taken = 0.474sec)  (Time taken for logging = 2.100sec)\n",
            "[Step 91/400] Training loss = 0.05375  (Time taken = 0.514sec)  (Time taken for logging = 2.156sec)\n",
            "[Step 96/400] Training loss = 0.04645  (Time taken = 0.496sec)  (Time taken for logging = 2.490sec)\n",
            "Checkpoint created after 100 steps\n",
            "[Step 101/400] Training loss = 0.04224  (Time taken = 0.477sec)  (Time taken for logging = 2.522sec)\n",
            "[Step 106/400] Training loss = 0.04266  (Time taken = 0.468sec)  (Time taken for logging = 2.125sec)\n",
            "[Step 111/400] Training loss = 0.03695  (Time taken = 0.476sec)  (Time taken for logging = 2.136sec)\n",
            "[Step 116/400] Training loss = 0.03547  (Time taken = 0.461sec)  (Time taken for logging = 2.021sec)\n",
            "[Step 121/400] Training loss = 0.03259  (Time taken = 0.469sec)  (Time taken for logging = 2.060sec)\n",
            "[Step 126/400] Training loss = 0.03427  (Time taken = 0.474sec)  (Time taken for logging = 2.035sec)\n",
            "[Step 131/400] Training loss = 0.03310  (Time taken = 0.472sec)  (Time taken for logging = 2.039sec)\n",
            "[Step 136/400] Training loss = 0.02973  (Time taken = 0.517sec)  (Time taken for logging = 2.073sec)\n",
            "[Step 141/400] Training loss = 0.03112  (Time taken = 0.460sec)  (Time taken for logging = 2.068sec)\n",
            "[Step 146/400] Training loss = 0.02910  (Time taken = 0.463sec)  (Time taken for logging = 2.063sec)\n",
            "Checkpoint created after 150 steps\n",
            "[Step 151/400] Training loss = 0.03197  (Time taken = 0.472sec)  (Time taken for logging = 2.127sec)\n",
            "[Step 156/400] Training loss = 0.02745  (Time taken = 0.465sec)  (Time taken for logging = 2.089sec)\n",
            "[Step 161/400] Training loss = 0.02487  (Time taken = 0.472sec)  (Time taken for logging = 2.128sec)\n",
            "[Step 166/400] Training loss = 0.02521  (Time taken = 0.462sec)  (Time taken for logging = 2.006sec)\n",
            "[Step 171/400] Training loss = 0.02375  (Time taken = 0.478sec)  (Time taken for logging = 2.125sec)\n",
            "[Step 176/400] Training loss = 0.02376  (Time taken = 0.460sec)  (Time taken for logging = 2.074sec)\n",
            "[Step 181/400] Training loss = 0.02485  (Time taken = 0.515sec)  (Time taken for logging = 2.098sec)\n",
            "[Step 186/400] Training loss = 0.02125  (Time taken = 0.474sec)  (Time taken for logging = 2.030sec)\n",
            "[Step 191/400] Training loss = 0.02279  (Time taken = 0.471sec)  (Time taken for logging = 2.129sec)\n",
            "[Step 196/400] Training loss = 0.02249  (Time taken = 0.468sec)  (Time taken for logging = 2.072sec)\n",
            "Checkpoint created after 200 steps\n",
            "[Step 201/400] Training loss = 0.02113  (Time taken = 0.466sec)  (Time taken for logging = 2.145sec)\n",
            "[Step 206/400] Training loss = 0.02003  (Time taken = 0.468sec)  (Time taken for logging = 2.066sec)\n",
            "[Step 211/400] Training loss = 0.01955  (Time taken = 0.462sec)  (Time taken for logging = 2.080sec)\n",
            "[Step 216/400] Training loss = 0.01920  (Time taken = 0.465sec)  (Time taken for logging = 2.043sec)\n",
            "[Step 221/400] Training loss = 0.01792  (Time taken = 0.527sec)  (Time taken for logging = 2.149sec)\n",
            "[Step 226/400] Training loss = 0.02165  (Time taken = 0.516sec)  (Time taken for logging = 2.123sec)\n",
            "[Step 231/400] Training loss = 0.01823  (Time taken = 0.461sec)  (Time taken for logging = 2.087sec)\n",
            "[Step 236/400] Training loss = 0.01654  (Time taken = 0.467sec)  (Time taken for logging = 2.110sec)\n",
            "[Step 241/400] Training loss = 0.01780  (Time taken = 0.466sec)  (Time taken for logging = 2.124sec)\n",
            "[Step 246/400] Training loss = 0.01806  (Time taken = 0.466sec)  (Time taken for logging = 2.156sec)\n",
            "Checkpoint created after 250 steps\n",
            "[Step 251/400] Training loss = 0.01875  (Time taken = 0.482sec)  (Time taken for logging = 2.061sec)\n",
            "[Step 256/400] Training loss = 0.01706  (Time taken = 0.470sec)  (Time taken for logging = 2.383sec)\n",
            "[Step 261/400] Training loss = 0.01391  (Time taken = 0.638sec)  (Time taken for logging = 2.166sec)\n",
            "[Step 266/400] Training loss = 0.01588  (Time taken = 0.471sec)  (Time taken for logging = 2.124sec)\n",
            "[Step 271/400] Training loss = 0.01779  (Time taken = 0.516sec)  (Time taken for logging = 2.093sec)\n",
            "[Step 276/400] Training loss = 0.01528  (Time taken = 0.444sec)  (Time taken for logging = 2.022sec)\n",
            "[Step 281/400] Training loss = 0.01544  (Time taken = 0.469sec)  (Time taken for logging = 2.054sec)\n",
            "[Step 286/400] Training loss = 0.01360  (Time taken = 0.473sec)  (Time taken for logging = 2.051sec)\n",
            "[Step 291/400] Training loss = 0.01374  (Time taken = 0.469sec)  (Time taken for logging = 2.121sec)\n",
            "[Step 296/400] Training loss = 0.01387  (Time taken = 0.470sec)  (Time taken for logging = 2.029sec)\n",
            "Checkpoint created after 300 steps\n",
            "[Step 301/400] Training loss = 0.01103  (Time taken = 0.461sec)  (Time taken for logging = 2.161sec)\n",
            "[Step 306/400] Training loss = 0.01506  (Time taken = 0.466sec)  (Time taken for logging = 2.090sec)\n",
            "[Step 311/400] Training loss = 0.01198  (Time taken = 0.460sec)  (Time taken for logging = 2.107sec)\n",
            "[Step 316/400] Training loss = 0.01250  (Time taken = 0.516sec)  (Time taken for logging = 2.063sec)\n",
            "[Step 321/400] Training loss = 0.01392  (Time taken = 0.481sec)  (Time taken for logging = 2.051sec)\n",
            "[Step 326/400] Training loss = 0.01201  (Time taken = 0.466sec)  (Time taken for logging = 2.070sec)\n",
            "[Step 331/400] Training loss = 0.01330  (Time taken = 0.465sec)  (Time taken for logging = 2.056sec)\n",
            "[Step 336/400] Training loss = 0.01378  (Time taken = 0.458sec)  (Time taken for logging = 2.039sec)\n",
            "[Step 341/400] Training loss = 0.01255  (Time taken = 0.465sec)  (Time taken for logging = 2.067sec)\n",
            "[Step 346/400] Training loss = 0.01218  (Time taken = 0.468sec)  (Time taken for logging = 2.158sec)\n",
            "Checkpoint created after 350 steps\n",
            "[Step 351/400] Training loss = 0.01303  (Time taken = 0.481sec)  (Time taken for logging = 2.147sec)\n",
            "[Step 356/400] Training loss = 0.01158  (Time taken = 0.470sec)  (Time taken for logging = 2.107sec)\n",
            "[Step 361/400] Training loss = 0.01168  (Time taken = 0.518sec)  (Time taken for logging = 2.036sec)\n",
            "[Step 366/400] Training loss = 0.01112  (Time taken = 0.473sec)  (Time taken for logging = 2.082sec)\n",
            "[Step 371/400] Training loss = 0.01128  (Time taken = 0.477sec)  (Time taken for logging = 2.129sec)\n",
            "[Step 376/400] Training loss = 0.01261  (Time taken = 0.466sec)  (Time taken for logging = 2.246sec)\n",
            "[Step 381/400] Training loss = 0.01184  (Time taken = 0.467sec)  (Time taken for logging = 2.091sec)\n",
            "[Step 386/400] Training loss = 0.01071  (Time taken = 0.465sec)  (Time taken for logging = 2.125sec)\n",
            "[Step 391/400] Training loss = 0.00925  (Time taken = 0.466sec)  (Time taken for logging = 2.123sec)\n",
            "[Step 396/400] Training loss = 0.01136  (Time taken = 0.475sec)  (Time taken for logging = 2.062sec)\n",
            "Checkpoint created after 400 steps\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}