# Learning-Overparametrized-Neural-Nets

In this work, I have tried to reproduce the experimental results shown in the paper - [Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data](https://arxiv.org/pdf/1808.01204.pdf). 

In the paper, the authors have analyzed a two-layer neural network, that is trained by keeping the weights of second layer fixed. The network was trained for classification task on a synthetic ”structured” dataset generated by sampling from a mixture of well separated clustered distributions (like gaussian). Any subset of components from the mixture can correspond to one class. The authors have shown that in this setting the activation pattern at the hidden layer does not change much from its state during initialization and that the weights learned after training are quite close to the initial weights. Under this condition, the gradients of a pseudo network - a network for which activation patterns in the hidden layer is kept fixed, and that of the true network can be coupled. Using this as a base, the authors have derived results for convergence of Stochastic Gradient Descent method used for training the neural network. They have also shown that the solution to which SGD converges generalizes well in the test dataset. In this work, I have implemented both true and pseudo neural network model using TensorFlow framework in Python.

Originaly implemented and tested on [Google Colab](https://colab.research.google.com/drive/1CPxHUiG7HpHAbAT6U_f_3qgXCpCMnT6G), the code is best utilized when viewed on Colab.
